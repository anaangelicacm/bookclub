{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-terminology",
   "metadata": {},
   "source": [
    "# Book Club"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-norfolk",
   "metadata": {},
   "source": [
    "## LIBRARIES AND SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vulnerable-blond",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T00:49:26.312541Z",
     "start_time": "2022-06-17T00:49:12.327963Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "color-turning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T00:49:26.335656Z",
     "start_time": "2022-06-17T00:49:26.312541Z"
    }
   },
   "outputs": [],
   "source": [
    "# simulates a browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebkit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "# home page\n",
    "url = 'https://books.toscrape.com/index.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-danish",
   "metadata": {},
   "source": [
    "## 1. DATA LOADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-market",
   "metadata": {},
   "source": [
    "### 1.1. Extraction the number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "major-smile",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T00:49:28.281543Z",
     "start_time": "2022-06-17T00:49:26.351669Z"
    }
   },
   "outputs": [],
   "source": [
    "# API request\n",
    "page = requests.get(url, headers = headers)\n",
    "\n",
    "# transform the html request into a beautiful soup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# select the total number of pages\n",
    "number_page = int(soup.find('li', class_ = 'current').text.split('of')[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-resistance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T02:41:10.922882Z",
     "start_time": "2022-06-13T02:41:10.878002Z"
    }
   },
   "source": [
    "### 1.2. Extraction of details page links for all books on the site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ready-triumph",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T00:51:53.611969Z",
     "start_time": "2022-06-17T00:49:28.281543Z"
    }
   },
   "outputs": [],
   "source": [
    "# creates the dataframe structure\n",
    "df_books = pd.DataFrame(columns = ['link'])\n",
    "\n",
    "for i in range(1, number_page + 1):\n",
    "    # pagination\n",
    "    url = 'https://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "    \n",
    "    # API request\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    # transform the html request into a beautiful soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # identifies the book\n",
    "    book = soup.find_all('li', class_ = 'col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "    \n",
    "    # extract the details url\n",
    "    path = 'https://books.toscrape.com/catalogue/'\n",
    "    book = [path + i.find('a')['href'] for i in book]\n",
    "    \n",
    "    # auxiliary dataframe \n",
    "    df_aux = pd.DataFrame(columns = ['link'])\n",
    "    df_aux['link'] = book\n",
    "    \n",
    "    # contacts collected informations in a single dataframe\n",
    "    df_books = pd.concat([df_books, df_aux], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-limitation",
   "metadata": {},
   "source": [
    "### 1.3. Extract information from each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "static-qualification",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T01:11:21.574268Z",
     "start_time": "2022-06-17T00:51:53.611969Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in df_books.loc[:, 'link']:\n",
    "    # API request\n",
    "    page = requests.get(i, headers = headers)\n",
    "\n",
    "    # transform the html request into a beautiful soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        \n",
    "    # informations\n",
    "    category = soup.find('ul', class_ = 'breadcrumb').find_all('a')[2].get_text()\n",
    "    title = soup.find('div', class_ = 'col-sm-6 product_main').find('h1').get_text()\n",
    "    price = soup.find('div', class_ = 'col-sm-6 product_main').find('p', class_ = 'price_color').get_text()\n",
    "    availability = soup.find('div', class_ = 'col-sm-6 product_main').find('p', class_ = 'instock availability').get_text().strip()\n",
    "    rating = soup.find('div', class_ = 'col-sm-6 product_main').find_all('p')[2]['class'][-1]\n",
    "    ucp = soup.find('table', class_ = 'table table-striped').find('td').get_text()\n",
    "\n",
    "    # index\n",
    "    line = df_books[df_books['link'] == i].index\n",
    "    \n",
    "    # put the data in the dataframe\n",
    "    df_books.loc[line, 'category'] = category\n",
    "    df_books.loc[line, 'title'] = title\n",
    "    df_books.loc[line, 'price'] = price\n",
    "    df_books.loc[line, 'availability'] = availability\n",
    "    df_books.loc[line, 'rating'] = rating\n",
    "    df_books.loc[line, 'ucp'] = ucp\n",
    "    df_books['web_scraping_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-sierra",
   "metadata": {},
   "source": [
    "## 2. DATA CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-precipitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-17T01:11:21.684973Z",
     "start_time": "2022-06-17T01:11:21.574268Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- PRICE \n",
    "# deletes strange character\n",
    "df_books['price'] = df_books['price'].apply(lambda x: re.search('\\d+\\.\\d+', x ).group(0) if pd.notnull(x) else x)\n",
    "\n",
    "# converts the type of the variable\n",
    "df_books['price'] = df_books['price'].astype(float)\n",
    "    \n",
    "# --- AVAILABILITY\n",
    "# extracts the number\n",
    "df_books['availability'] = df_books['availability'].apply(lambda x: re.search('\\d+', x).group(0) if pd.notnull(x) else x)\n",
    "\n",
    "# converts the type of the variable\n",
    "df_books['availability'] = df_books['availability'].astype(int)\n",
    "    \n",
    "# --- RATING\n",
    "# changes word to number\n",
    "df_books['rating'] = df_books['rating'].apply(lambda x: 1 if x == 'One' else\n",
    "                                                        2 if x == 'Two' else\n",
    "                                                        3 if x == 'Three' else\n",
    "                                                        4 if x == 'Four' else 5)\n",
    "\n",
    "# converts the type of the variable\n",
    "df_books['rating'] = df_books['rating'].astype(int)\n",
    "    \n",
    "# --- DATE\n",
    "# converts the type of the variable\n",
    "df_books['web_scraping_date'] = pd.to_datetime(df_books['web_scraping_date'])\n",
    "    \n",
    "# --- LINK\n",
    "# drops the feature link\n",
    "df_books = df_books.drop('link', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-cemetery",
   "metadata": {},
   "source": [
    "## 3. DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-swaziland",
   "metadata": {},
   "source": [
    "### 3.1. Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emotional-hungary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T18:05:32.015636Z",
     "start_time": "2022-06-14T18:05:31.746350Z"
    }
   },
   "outputs": [],
   "source": [
    "query_bookclub_schema = \"\"\"\n",
    "                            CREATE TABLE IF NOT EXISTS books(\n",
    "                                   ucp               TEXT,             \n",
    "                                   category          TEXT,\n",
    "                                   title             TEXT,\n",
    "                                   price             REAL,\n",
    "                                   rating            INTEGER,\n",
    "                                   availability      INTEGER,\n",
    "                                   web_scraping_date TEXT\n",
    "                        );\n",
    "                        \"\"\"\n",
    "\n",
    "# create table\n",
    "conn = sqlite3.connect('../data/database_bookclub.sqlite')\n",
    "cursor = conn.execute(query_bookclub_schema)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-weekly",
   "metadata": {},
   "source": [
    "### 3.2. Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "raising-letters",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-14T18:06:08.910096Z",
     "start_time": "2022-06-14T18:06:08.634923Z"
    }
   },
   "outputs": [],
   "source": [
    "# organize the table\n",
    "data_insert = df_books[['ucp', 'category', 'title', 'price', 'rating', 'availability', 'web_scraping_date']].copy()\n",
    "    \n",
    "# create database connection\n",
    "conn = sqlite3.connect('../data/database_bookclub.sqlite')\n",
    "\n",
    "# insert data\n",
    "data_insert.to_sql('books', con = conn, if_exists = 'append', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
